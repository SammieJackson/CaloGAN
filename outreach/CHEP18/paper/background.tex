\subsection{Background: from GAN to conditional WGAN}

Generative Adversarial Networks (GANs) were originally presented by I.~Goodfellow~\etal in 2014 \cite{goodfellow2014generative} and quickly became a state-of-the-art technique in areas such as image generation \cite{radford2015unsupervised} with huge number of extensions \todo{\cite{1,2,3,4}}.


In the GAN framework the aim is to learn a \textit{generator} $G$ to warp an easy-to-draw distribution $p(\vz)$ (e.g. $p(\vz) = \mathcal{N}(0, I)$ ) into a target distribution $\pdata(\vx)$ to facilitate sampling from $\pdata(\vx)$. When $G$ is learned $G \equiv G^*$, sampling from the target distribution $\pdata(\vx)$ is done by first drawing a sample from the distribution $p(\vz)$ and then feeding the sample into the generator: $G^*(\vz) \sim \pdata$, where $\vz \sim p(\vz)$. For such sampling procedure, the time needed to draw a sample from $\pdata(\vx)$ is approximately equal to the time needed to evaluate the function $G$ in a point.  

The generator is learned by using a feedback from an external classifier (usually called \textit{discriminator}), which tries to find discrepancy between the target distribution $\pdata(\vx)$ and fake distribution $\pfake(\vx)$ defined by samples from the generator $G(\vz) \sim \pfake,\, \vz \sim p(\vz)$. The process in summarized in~\cref{fig:GANs}.

\begin{figure}
\centering
\includegraphics[width=0.3\linewidth]{figures/gan_pic.pdf}
\caption{Generative Adversarial Networks for digit generation. The generator $G$ transforms the noise vector $\vz \sim p(z)$ to an image of a digit and the discriminator $D$ classifies inputs as real digits or fake digits from generator. Generator and discriminator are trained in an adversarial manner: the task of $G$ is to make it impossible for $D$ to distinguish between the real and fake digits as in this case $G$ reproduces the data distribution $\pdata$.}\label{fig:GANs}
\end{figure}


More formally, generator $G$ and discriminator $D$ play the following zero sum game: 
\begin{equation}\label{eq:gan}
\min_G \max_D \E_{\vx \sim \pdata(\vx)} [\log D(\vx)] + \E_{\vx \sim \pfake(\vx)} [\log(1 - D(\vx))],
\end{equation} 
where $D(G(\vz))$ is the output of the discriminator specifying the probability of its input to come be sampled from the target distribution.

In practice $G$ and $D$ are parametrized by deep neural networks and the objective~\cref{eq:gan} is optimized using alternating gradient descent. For a fixed generator, the discriminator minimizes binary cross-entropy in a binary classification problem (samples from target distribution versus samples from fake distribution). For a fixed discriminator, generator is updated to make its samples to be misclassified by discriminator, thus moving fake distribution closer to the target.   

It is possible to show, that for a fixed generator the optimal value for the inner optimization can be written analytically: 
\begin{equation}\label{eq:js}
\max_D \E_{\vx \sim \pdata(\vx)} [\log D(\vx)] + \E_{\vx \sim \pfake(\vx)} [\log(1 - D(\vx))] = \text{JS}( \pdata \dsep \pfake)
\end{equation} 
where $\text{JS}$ is Jenson-Shennon divergence. In fact, for a fixed generator (hence fake distribution), the discriminator computes the divergence between the target and fake distributions. When the divergence is computed, the generator aims to update fake distribution to make this divergence lower: $\min_G \text{JS}( \pdata \dsep \pfake )$. While Jenson-Shennon divergence naturally arises from the original~\cref{eq:gan}, any divergence or distance $\mathcal{D}$ can be used instead: $\min_G \mathcal{D}( \pdata \dsep \pfake )$.

A recent work~\cite{arjovsky2017wasserstein} proposed to use \textit{Wasserstein distance} and proved ... 

\begin{equation}\label{eq:wasserstein_metric}
W(\pdata \dsep \pfake ) = \max_{f \in \mathcal{F}} \E_{\vx \sim \pdata(\vx)}[f(\vx)] - \E_{\vx \sim \pfake} [f(\vx)]
\end{equation}
where $\mathcal{F}$ is a set of 1-Lipshitz functions. Using Wasserstein distance instead of Jenson-Shennon divergence in a GAN objective leads to the following game: 

\begin{equation}\label{eq:wgan}
\min_G \max_{f \in \mathcal{F}} \E_{\vx \sim \pdata(\vx)}[f(\vx)] - \E_{\vx \sim \pfake(\vx)} [f(\vx)]
\end{equation}

It is highly non-trivial to search over a set of 1-Lipshitz functions and several ways has been proposed in order to force this constraint \cite{arjovsky2017wasserstein,gulrajani2017improved}. \cite{gulrajani2017improved} proved, that among optimal functions for~\cref{eq:wasserstein_metric} there exists such, that norm of it's gradient in any point equals one. To make it practical this constraint is relaxed and added to the main objective while $f$ is searched over all possible functions:    

% When parametri
% \begin{equation}\label{gpwgan-loss}
% \begin{gathered}
% \mathcal{L}(\bm{\theta}) =
% \underbrace{ \underset{\vx \sim \pdata(\vx)}{\E}  \Big[f(\tilde{\vx})\Big] - \underset{\vx \sim \mathbb{P}_r}{\mathbb{E}} \Big[D(\vx)\Big]}_{\text{original loss}} + 
% \underbrace{ \lambda \underset{\vx \sim \mathbb{P}_g}{\mathbb{E}} \Big[\big(\|\nabla_{\tilde{\vx}} D(\tilde{\vx})\|_2 - 1\big)^2 \Big]}_{\text{gradient penalty}}.
% \end{gathered}
% \end{equation}

\begin{equation}\label{gpwgan-loss}
\begin{gathered}
\min_G \max_f \E_{\vx \sim \pdata(\vx)}  f(\vx) - \E_{\vx \sim \pfake(\vx)} f(\vx) + 
\lambda \E_{\vx \sim \pfake} \big(\|\nabla_{\tilde{\vx}} D(\tilde{\vx})\|_2 - 1\big)^2 .
\end{gathered}
\end{equation}


% \subsubsection{Wasserstein GAN}
% There are various modifications of GANs for struggling with some typical problems and for improving the training procedure. A common GANs issue is so--called mode collapse when $p_\text{model} (\vx)$ fails to capture a multimodal nature of $\pdata(\vx)$ and in extreme cases all the generated samples might be identical, in more involved architectures such as Waserstain GAN \cite{arjovsky2017wasserstein} the discriminator loss is argued to be consistent with the image quality. The main idea is to apply the Wasserstein-1 distance in order to compare $\pdata$ and $p_{\text{model}}$.

% Let $\mathbb{X}$ be a compact metric set and let $\mathbb{B}$ denote the set of all the Borel subsets of $\mathbb{X}$. Let $Prob(\mathbb{X})$ denote the space of probability measures defined on $\mathbb{X}$. The \emph{Earth-Mover} or \emph{Wasserstein-1} distance between two distributions $\mathbb{P}_r, \mathbb{P}_g \in Prob(\mathbb{X})$ is defined in the following way:

% \begin{equation}\label{wasserstein_metric}
% W(\mathbb{P}_r, \mathbb{P}_g) = \inf_{\gamma \in \Pi(\mathbb{P}_r, \mathbb{P}_g)} \mathbb{E}_{(x, y) \sim \gamma} \big{[}\|\vx-\textbf{y}\|\big{]},
% \end{equation}

% where $\Pi(\mathbb{P}_r, \mathbb{P}_g)$ is the set of all joint distributions $\gamma(x, y)$ whose marginals are respectively $\mathbb{P}_r$ and $\mathbb{P}_g$. $\gamma(x, y)$ denotes how much “mass” must be transported from $x$ to $y$ in order to transform the distributions $\mathbb{P}_r$ into the distribution $\mathbb{P}_g$. Thus, the Wasserstein distance can be defined as the minimum cost of transporting mass in order to transform the distribution $\mathbb{P}_r$ into the distribution $\mathbb{P}_q$. 

% By applying to this distance the \emph{Kullback--Leibler divergence}, the \emph{Jensen-Shannon divergence} and the Kantorovich--Rubinstein duality and also parameterization of family of functions $\{f_w\}_{w \in \mathcal{W}}$ that are all K-Lipschitz for some K we could consider solving the problem (for more detailed information see the paper \cite{arjovsky2017wasserstein})

% \begin{equation}\label{optim}
% \max_{w \in \mathcal{W}} \mathbb{E}_{x \sim P_r}[f_w(x)] - \mathbb{E}_{z\sim p(z)} [f_w(g_\theta(z))]
% \end{equation}

% and if the supremum in \eqref{optim} is attained for some $w \in \mathcal{W}$, this process would lead to a calculation of $W(\mathbb{P}_r, \mathbb{P}_\theta)$ up to a multiplicative constant.
% So, the WGAN value function is
% \begin{equation}\label{wgan_loss}
% \min_G \max_{D \in \mathcal{D}}  \mathbb{E}_{\vx \sim \mathbb{P}_r}  [D(\vx)] - \mathbb{E}_{\tilde{\vx} \sim \mathbb{P}_g} [D(\tilde{\vx})],
% \end{equation}
% where $\mathcal{D}$ is the set of 1-Lipschitz functions and $\mathbb{P}_g$ is  the model distribution defined by $\tilde{\vx} = G(\vz), ~\vz \sim p(\vz).$

% In general, the training procedure of GANs is known to be difficult and presents such issues as mode collapse, and WGAN often helps to overcome this problem.

% The Wasserstein GAN value function makes optimization process easier because the discriminator's gradient with respect to its input is better behaved than its GAN counterpart. Also, it was noticed that the WGAN value function tend to correlate with the original data quality which is not always the case for ordinary GANs.


% \subsubsection{Wasserstain GAN with gradient penalty}
% An alternative way to enforce the Lipschitz constraint was introduced in \cite{gulrajani2017improved}. The authors consider directly constraining the gradient norm of the discriminator's output with respect to its input because a differentiable function is 1-Lipschtiz if and only if it has gradients with norm at most 1 everywhere. To come over tractability issues a soft version of the constraint with a penalty on the gradient norm for random samples $\tilde{\vx} \sim \mathbb{P}_{\tilde{\vx}}$ was suggested. Therefore, a new obtained objective is 
% \begin{equation} \label{gpwgan-loss}
% \begin{gathered}
% \mathcal{L}(\bm{\theta}) =
% \underbrace{ \underset{\tilde{\vx} \sim \mathbb{P}_g}{\mathbb{E}}  \Big[D(\tilde{\vx})\Big] - \underset{\vx \sim \mathbb{P}_r}{\mathbb{E}} \Big[D(\vx)\Big]}_{\text{original loss}} + 
% \underbrace{ \lambda \underset{\vx \sim \mathbb{P}_g}{\mathbb{E}} \Big[\big(\|\nabla_{\tilde{\vx}} D(\tilde{\vx})\|_2 - 1\big)^2 \Big]}_{\text{gradient penalty}}.
% \end{gathered}
% \end{equation}

% This approach demonstrates strong modeling performance and stability across a variety of architectures.  Now it is a state-of-the-art technique in GANs. 
% How to calculate this loss in practice is described on~\cref{sec:training_strategy}.

\subsection{GANs in high energy physics}
The first \todo{strong claim! is it 100\% true? what about LAGAN https://arxiv.org/pdf/1701.05927.pdf?} systematic study on the application of deep learning to particle physics has been carried out by Paganini et al. in 2017 \cite{paganini2017calogan} and called CaloGAN. The authors aim to speedup particle simulation in a 3-layer unheterogeneous calorimeter at the LHC using GANs framework and achieve $\sim 10^5 \times$ speedup. They use an existing state-of-the-art (but slow) simulation engine \geant to create a training dataset. They simulated positrons, photons and charged pions with various energies from 1 GeV to 100 GeV \todo{with energies are incident perpendicular on the center of the calorimeter front}. The shower in the first layer is represented as a $3 \times 96$ image, the middle layer as a $12 \times 12$ image, and the last layer as a $12 \times 6$ image. 

Their design of the generator network is based on DCGAN structure \cite{radford2015unsupervised} with some convolutional layes replaced by locally-connected layers \cite{taigman2014deepface}. The idea of locally connected layers is based on the fact that every pixel position gets its own filter while an ordinary convolutional layer is applied over the whole image, independently of location. Spreading of this method to particle physics simulation has been described in the previous work of the authors and such type of neural network was called LAGAN \cite{de2017learning}. A special section in the paper is devoted to the evaluation of the quality of the CaloGAN produced images where  sparsity level,  energy per layer or total energy are suggested to measure the performance of the model. 

The obtained results demonstrate a prospect of application of GANs for the particle showers generation and replacing of the Monte Carlo methods with the proposed approach. The CaloGAN shows sizable simulation-time speed ups compared to \geant. 

In fact, the CaloGan model is based on DCGAN with the described tricks. However, GANs tend to suffer from mode collapse. Therefore, the CaloGan architecture cannot be applied for all datasets, because there is a high probability of mode collapse appearance and it is a limitation of this work.
